{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29b7a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# ========== PLOTTING ==========\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# 1. Heaviside step (1D)\n",
    "data = datasets[0]\n",
    "x_samples = generate_qmc_samples(1, 100, data['domain'])\n",
    "y_samples = f_heaviside(x_samples[:, 0])\n",
    "axes[0].scatter(x_samples[:, 0], y_samples, c='b', s=6, alpha=0.8)\n",
    "axes[0].set_title(data['name'])\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('f(x)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. sqrt(2-x) (1D)\n",
    "data = datasets[1]\n",
    "x_samples = generate_qmc_samples(1, 100, data['domain'])\n",
    "y_samples = f_gamma(x_samples[:, 0])\n",
    "axes[1].scatter(x_samples[:, 0], y_samples, c='b', s=6, alpha=0.8)\n",
    "axes[1].set_title(data['name'])\n",
    "axes[1].set_xlabel('x')\n",
    "axes[1].set_ylabel('f(x)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. 2D Gaussians \n",
    "data = datasets[2]  \n",
    "samples = generate_qmc_samples(2, 10000, data['domain'])\n",
    "X, Y = samples[:, 0], samples[:, 1]\n",
    "Z = f_gaussians(X, Y)\n",
    "scatter = axes[2].scatter(X, Y, c=Z, cmap='viridis', s=5)\n",
    "axes[2].set_title(data['name'])\n",
    "axes[2].set_xlabel('x')\n",
    "axes[2].set_ylabel('y')\n",
    "plt.colorbar(scatter, ax=axes[2])\n",
    "\n",
    "# 4. Rosenbrock\n",
    "data = datasets[3]\n",
    "samples = generate_qmc_samples(2, 10000, data['domain'])\n",
    "X, Y = samples[:, 0], samples[:, 1]\n",
    "Z = f_rosenbrock(X, Y)\n",
    "Z_log = jnp.log10(Z + 1)\n",
    "scatter = axes[3].scatter(X, Y, c=Z_log, cmap='plasma', s=5)\n",
    "axes[3].set_title(data['name'] + ' (log scale)')\n",
    "axes[3].set_xlabel('x')\n",
    "axes[3].set_ylabel('y')\n",
    "plt.colorbar(scatter, ax=axes[3])\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2be3e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Main Execution ==========\n",
    "start_time = time.time() # Start timing\n",
    "\n",
    "architectures = {\n",
    "    'shallow': lambda dim: [dim, 40, 1],  # Shallow network with one hidden layer\n",
    "    'deep': lambda dim: [dim, 20, 20, 20, 1],  # Deep network with three hidden layers\n",
    "}\n",
    "activations = {\n",
    "    'relu': nn.relu,\n",
    "    'tanh': nn.tanh,\n",
    "}\n",
    "\n",
    "# Choose dataset\n",
    "#selected_dataset = datasets['heaviside']\n",
    "#selected_dataset = datasets['gamma']\n",
    "#selected_dataset = datasets['gaussians']\n",
    "selected_dataset = datasets['rosenbrock']\n",
    "\n",
    "dim = selected_dataset['dim']\n",
    "domain = selected_dataset['domain']\n",
    "func = selected_dataset['func']\n",
    "name = selected_dataset['name']\n",
    "\n",
    "data_points=100 if dim == 1 else 10000\n",
    "test_points=100\n",
    "\n",
    "x_data = generate_qmc_samples(dim, data_points, domain, seed=0)\n",
    "x_test = generate_qmc_samples(dim, test_points, domain, seed=1)\n",
    "x_test_norm = normalize(x_test, domain)\n",
    "\n",
    "y_data = func(x_data[:, 0]) if dim == 1 else func(*x_data.T[:dim])\n",
    "y_test = func(x_test[:, 0]) if dim == 1 else func(*x_test.T[:dim])\n",
    "\n",
    "trained_models = {}\n",
    "best_err = float('inf')\n",
    "best_setting = None\n",
    "for arch_name, arch_func in architectures.items():\n",
    "    sizes = arch_func(dim)\n",
    "    for act_name, act_func in activations.items():\n",
    "        print(f\"Training {name} with {arch_name} {act_name}\")\n",
    "        params, loss_history = train_model(x_test_norm, y_test, size, act_name, update_freq=1000)\n",
    "        y_pred = predict(params, x_test_norm, act_name)\n",
    "\n",
    "        abs_err = jnp.abs(y_pred - y_test)\n",
    "        rel_err = abs_err / (jnp.abs(y_test) + 1e-10)\n",
    "        mean_abs = jnp.mean(abs_err)\n",
    "        mean_rel = jnp.mean(rel_err)\n",
    "        print(f\"{name} {arch_name} {act_name} - Mean Abs Err: {mean_abs:.6f}, Mean Rel Err: {mean_rel:.6f}\")\n",
    "        if mean_abs < best_err:\n",
    "            best_err = mean_abs\n",
    "            best_setting = (arch_name, act_name)\n",
    "        trained_models[f\"{arch_name}_{act_name}\"] = {'params': params, 'y_pred': y_pred, 'x_data': x_data, 'y_data': y_data}\n",
    "\n",
    "# Save trained models and data for visualization\n",
    "with open(f\"{name}_trained_models.pkl\", 'wb') as f:\n",
    "    pickle.dump(trained_models, f)\n",
    "\n",
    "print(f\"Best setting for {name}: {best_setting} with Mean Abs Err: {best_err:.6f}\")\n",
    "print(\"Comments: Shallow ReLU suits Heaviside's piecewise nature. Deep Tanh excels with smooth functions like Gaussians and Rosenbrock due to better non-linearity capture.\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time} seconds\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
